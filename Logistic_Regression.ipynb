{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVnOrpHggo0YyfbM6lNVaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushpitab18/PW_MACHINE_LEARNING_ASSIGNMENTS/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\"Logistic Regression : \"\n",
        "\n",
        "##\"Theoretical Questions and Answers : \""
      ],
      "metadata": {
        "id": "XRN29qcfmYqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1 : What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "\n",
        "### ans :  \n",
        "\n",
        "####- Logistic Regression :\n",
        "\n",
        "Logistic Regression, also known as Logit Regression, is a classification algorithm that predicts a binary outcome (0/1, yes/no, etc.) based on one or more input features.\n",
        "\n",
        "####- Linear Regression :\n",
        "\n",
        "Linear Regression is a regression algorithm that predicts a continuous output variable based on one or more input features. The goal is to establish a linear relationship between the inputs and the output.\n",
        "\n",
        "\n",
        "####- Key Differences:\n",
        "\n",
        "1. Outcome Variable: Linear Regression predicts continuous outcomes, while Logistic Regression predicts binary outcomes.\n",
        "2. Relationship Assumption: Linear Regression assumes a linear relationship, whereas Logistic Regression assumes a non-linear, sigmoidal relationship.\n",
        "3. Optimization Technique: Linear Regression uses OLS, while Logistic Regression uses MLE.\n",
        "4. Prediction Output: Linear Regression outputs a continuous value, whereas Logistic Regression outputs a probability between 0 and 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FqmaAsw9nbYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2 : What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :   \n",
        "\n",
        "\n",
        "####The mathematical equation for Logistic Regression is:\n",
        "\n",
        "- p = 1 / (1 + e^(-z))\n",
        "\n",
        "####- where:\n",
        "\n",
        "p = probability of the positive class (e.g., 1, yes, etc.)\n",
        "\n",
        "e = base of the natural logarithm (approximately 2.718)\n",
        "\n",
        "z = linear combination of input features and weights\n",
        "\n",
        "- z = w0 + w1x1 + w2x2 + … + wnxn\n",
        "\n",
        "####- where:\n",
        "\n",
        "w0 = bias term\n",
        "\n",
        "wi = weights for each input feature xi\n",
        "\n",
        "xi = input features\n",
        "\n",
        "This equation is also known as the sigmoid function or logistic function."
      ],
      "metadata": {
        "id": "Qq7UGl2-ozlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3 : Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "###ans :  \n",
        "\n",
        "####We use the Sigmoid function in Logistic Regression for several reasons:\n",
        "\n",
        "####- Mathematical Properties :\n",
        "\n",
        "1. Maps to a probability: The Sigmoid function maps any real-valued number to a value between 0 and 1, which is ideal for representing probabilities.\n",
        "2. Continuous and differentiable: The Sigmoid function is continuous and differentiable, making it easy to optimize using gradient-based methods.\n",
        "\n",
        "####- Interpretability :\n",
        "\n",
        "1. Easy to interpret: The output of the Sigmoid function can be interpreted as a probability, which is intuitive and easy to understand.\n",
        "2. Odds ratio: The Sigmoid function can be used to model the odds ratio, which is a measure of the strength of association between a predictor and the outcome.\n",
        "\n",
        "####- Computational Efficiency :\n",
        "\n",
        "1. Fast computation: The Sigmoid function is computationally efficient to evaluate, making it suitable for large-scale logistic regression problems.\n",
        "\n",
        "####- Statistical Properties :\n",
        "\n",
        "1. Maximum likelihood estimation: The Sigmoid function is used in logistic regression to model the probability of the positive class, which is essential for maximum likelihood estimation.\n",
        "\n",
        "Overall, the Sigmoid function provides a convenient and interpretable way to model probabilities in logistic regression, while also being computationally efficient and statistically sound."
      ],
      "metadata": {
        "id": "jinAXGNHrvL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4 : What is the cost function of Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :  \n",
        "\n",
        "####The cost function of Logistic Regression is called the Log Loss or Cross-Entropy Loss. It is defined as:\n",
        "\n",
        "- J(θ) = -[y * log(h(x)) + (1-y) * log(1-h(x))]\n",
        "\n",
        "####- where :\n",
        "\n",
        "θ: model parameters\n",
        "\n",
        "y: true label (0 or 1)\n",
        "\n",
        "h(x): predicted probability of the positive class\n",
        "\n",
        "x: input features\n",
        "\n",
        "The Log Loss function measures the difference between the predicted probabilities and the true labels. The goal of logistic regression is to minimize the Log Loss function.\n",
        "\n",
        "The Log Loss function is widely used in classification problems, including logistic regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "eHmKYwSntxX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5 :  What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "#### Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. The penalty term is proportional to the magnitude of the model's coefficients.\n",
        "\n",
        "####Why is Regularization needed?\n",
        "\n",
        "1. Overfitting: Without regularization, logistic regression models can become too complex and fit the noise in the training data, resulting in poor performance on new, unseen data.\n",
        "2. Feature selection: Regularization can help select the most relevant features by shrinking the coefficients of irrelevant features.\n",
        "3. Model interpretability: Regularization can improve model interpretability by reducing the impact of correlated features.\n",
        "\n",
        "Regularization is a crucial technique in logistic regression, as it helps prevent overfitting, improves model interpretability, and selects the most relevant features.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sb6o8x_Iu4xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6 : Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "\n",
        "###ans :  \n",
        "\n",
        "Lasso, Ridge, and Elastic Net are regularization techniques used in linear regression to prevent overfitting. The main difference between them lies in the way they penalize the model's coefficients.\n",
        "\n",
        "####- Lasso Regression (L1 Regularization) :\n",
        "\n",
        "- Penalty term: Absolute value of the coefficients (|β|).\n",
        "- Effect: Shrinks some coefficients to zero, effectively performing feature selection.\n",
        "- Use case: When there are many irrelevant features, and you want to select the most important ones.\n",
        "\n",
        "####- Ridge Regression (L2 Regularization) :\n",
        "\n",
        "- Penalty term: Square of the coefficients (β²).\n",
        "- Effect: Shrinks all coefficients by a certain amount, but doesn't set any to zero.\n",
        "- Use case: When all features are relevant, and you want to reduce the impact of correlated features.\n",
        "\n",
        "####- Elastic Net Regression :\n",
        "\n",
        "- Penalty term: Combination of L1 and L2 penalties (α|β| + (1-α)β²).\n",
        "- Effect: Balances between Lasso's feature selection and Ridge's coefficient shrinkage.\n",
        "- Use case: When you want to select important features and reduce the impact of correlated features.\n"
      ],
      "metadata": {
        "id": "9GncHB1EwJKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7 : When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "###ans :  \n",
        "\n",
        "####Use Elastic Net instead of Lasso or Ridge in the following situations:\n",
        "\n",
        "####- When to prefer Elastic Net over Lasso :\n",
        "\n",
        "1. Correlated features: When features are highly correlated, Lasso may arbitrarily select one feature over the other. Elastic Net's L2 penalty helps to reduce the impact of correlated features.\n",
        "2. Many relevant features: When there are many relevant features, Lasso may not be able to select all of them due to its L1 penalty. Elastic Net's combination of L1 and L2 penalties can help select multiple relevant features.\n",
        "\n",
        "####- When to prefer Elastic Net over Ridge :\n",
        "\n",
        "1. Feature selection: When feature selection is important, Elastic Net's L1 penalty can help identify the most relevant features, whereas Ridge regression may not perform feature selection.\n",
        "2. High-dimensional data: In high-dimensional data, Ridge regression may overfit due to the large number of features. Elastic Net's L1 penalty can help reduce the dimensionality and prevent overfitting.\n"
      ],
      "metadata": {
        "id": "EfVY04bMxhil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8 : What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "###ans :   \n",
        "\n",
        "####The regularization parameter\n",
        "\n",
        "𝜆(lambda) in Logistic Regression plays a crucial role in preventing overfitting and improving the model's generalization to new data. Here's how it impacts the model:\n",
        "\n",
        "####Regularization Type: In Logistic Regression, you can apply two types of regularization:\n",
        "\n",
        "####- L1 Regularization (Lasso):\n",
        "\n",
        "Encourages sparsity in the model by penalizing the absolute values of the coefficients. This can lead to some coefficients being exactly zero, effectively performing feature selection.\n",
        "\n",
        "####- L2 Regularization (Ridge):\n",
        "\n",
        "Penalizes the sum of the squared coefficients. This helps shrink the coefficients toward zero but doesn't eliminate them, leading to more stable and less sensitive predictions.\n",
        "\n",
        "####- Effects of λ :\n",
        "- Small λ (λ → 0) :\n",
        "1. Under-regularization: The model is not regularized enough, leading to overfitting.\n",
        "2. Large coefficients: The model's coefficients can become very large, making the model prone to overfitting.\n",
        "\n",
        "- Large λ (λ → ∞) :\n",
        "1. Over-regularization: The model is regularized too much, leading to underfitting.\n",
        "2. Small coefficients: The model's coefficients are shrunk towards zero, making the model too simple.\n"
      ],
      "metadata": {
        "id": "pIn9Qf8Qz8Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9 : What are the key assumptions of Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :  \n",
        "\n",
        "####Logistic Regression relies on the following key assumptions:\n",
        "\n",
        "1. Binary Dependent Variable :\n",
        "\n",
        "The dependent variable should be binary (0/1, yes/no, etc.).\n",
        "\n",
        "2. Independence of Observations :\n",
        "\n",
        "Each observation should be independent of the others.\n",
        "\n",
        "3. Linearity in the Log-Odds :\n",
        "\n",
        "The relationship between the independent variables and the log-odds of the dependent variable should be linear.\n",
        "\n",
        "4. No Multicollinearity :\n",
        "\n",
        "Independent variables should not be highly correlated with each other.\n",
        "\n",
        "5. Homoscedasticity:\n",
        "\n",
        "The variance of the residuals should be constant across all levels of the independent variables.\n",
        "\n",
        "6. No Outliers :\n",
        "\n",
        "There should be no outliers in the data.\n",
        "\n",
        "7. Correct Functional Form :\n",
        "\n",
        "The correct functional form of the relationship between the independent variables and the dependent variable should be specified.\n",
        "\n",
        "Violating these assumptions can lead to inaccurate predictions, biased coefficients, or incorrect inference."
      ],
      "metadata": {
        "id": "VYzFCrbx7om8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10 : What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "###ans :\n",
        "\n",
        "####Here are some popular alternatives to Logistic Regression for classification tasks:\n",
        "\n",
        "1. Decision Trees :\n",
        "\n",
        "Decision Trees are a simple, yet effective method for classification. They work by recursively partitioning the data into smaller subsets based on the features.\n",
        "\n",
        "2. Random Forests :\n",
        "\n",
        "Random Forests are an ensemble learning method that combines multiple Decision Trees to improve the accuracy and robustness of the model.\n",
        "\n",
        "3. Support Vector Machines (SVMs) :\n",
        "\n",
        "SVMs are a powerful classification method that can handle high-dimensional data. They work by finding the hyperplane that maximally separates the classes.\n",
        "\n",
        "4. K-Nearest Neighbors (KNN) :\n",
        "\n",
        "KNN is a simple, non-parametric method that classifies new instances based on the majority vote of their k-nearest neighbors.\n",
        "\n",
        "5. Naive Bayes :\n",
        "\n",
        "Naive Bayes is a probabilistic method that assumes independence between features. It's simple, efficient, and often surprisingly effective.\n",
        "\n",
        "6. Neural Networks :\n",
        "\n",
        "Neural Networks are a powerful class of models that can learn complex relationships between features. They're particularly useful for image and speech classification tasks.\n",
        "\n",
        "7. Gradient Boosting Machines (GBMs) :\n",
        "\n",
        "GBMs are an ensemble learning method that combines multiple weak models to create a strong predictive model.\n",
        "\n",
        "8. Extreme Gradient Boosting (XGBoost) :\n",
        "\n",
        "XGBoost is an optimized implementation of GBMs that's particularly effective for large-scale classification tasks.\n"
      ],
      "metadata": {
        "id": "xp28YowG8QLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11 :  What are Classification Evaluation Metrics?\n",
        "\n",
        "###ans :\n",
        "\n",
        "Classification Evaluation Metrics are used to assess the performance of a classification model. Here are some common metrics:\n",
        "\n",
        "####- Accuracy Metrics :\n",
        "1. Accuracy: Proportion of correctly classified instances.\n",
        "Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "2. Error Rate: Proportion of misclassified instances.\n",
        "Formula: Error Rate = (FP + FN) / (TP + TN + FP + FN)\n",
        "\n",
        "####- Class-Specific Metrics :\n",
        "1. Precision: Proportion of true positives among all predicted positive instances.\n",
        "Formula: Precision = TP / (TP + FP)\n",
        "2. Recall: Proportion of true positives among all actual positive instances.\n",
        "Formula: Recall = TP / (TP + FN)\n",
        "3. F1-score: Harmonic mean of precision and recall.\n",
        "Formula: F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "4. True Positive Rate (TPR): Same as recall.\n",
        "5. False Positive Rate (FPR): Proportion of false positives among all actual negative instances.\n",
        "Formula: FPR = FP / (TN + FP)\n",
        "\n",
        "####- Other Metrics :\n",
        "1. Area Under the Receiver Operating Characteristic Curve (AUC-ROC): Measures the model's ability to distinguish between positive and negative classes.\n",
        "2. Area Under the Precision-Recall Curve (AUC-PR): Measures the model's ability to detect positive instances.\n",
        "3. Confusion Matrix: A table that summarizes the predictions against the actual outcomes.\n"
      ],
      "metadata": {
        "id": "F5S9G-4-81j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12 : How does class imbalance affect Logistic Regression?\n",
        "\n",
        "###ans :\n",
        "\n",
        "Class imbalance occurs when the number of instances in one class significantly outweighs the number of instances in the other class(es). This can affect Logistic Regression in several ways:\n",
        "\n",
        "####- Effects on Logistic Regression :\n",
        "1. Biased coefficients: The model may assign more weight to the majority class, leading to biased coefficients.\n",
        "2. Poor predictive performance: The model may not generalize well to the minority class, resulting in poor predictive performance.\n",
        "3. Inaccurate probability estimates: The model may produce inaccurate probability estimates for the minority class.\n",
        "4. Overfitting: The model may overfit to the majority class, resulting in poor performance on new, unseen data.\n",
        "\n",
        "###- Addressing Class Imbalance :\n",
        "1. Oversampling the minority class: Create additional instances of the minority class to balance the dataset.\n",
        "2. Undersampling the majority class: Reduce the number of instances in the majority class to balance the dataset.\n",
        "3. SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic instances of the minority class using interpolation.\n",
        "4. Class weighting: Assign different weights to the classes during training, with higher weights for the minority class.\n",
        "5. Anomaly detection algorithms: Use algorithms specifically designed for anomaly detection, such as One-Class SVM or Local Outlier Factor (LOF).\n",
        "6. Ensemble methods: Use ensemble methods, such as Random Forest or Gradient Boosting, which can handle class imbalance.\n"
      ],
      "metadata": {
        "id": "WOXRTA4bBW8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13 : What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :  \n",
        "\n",
        "Hyperparameter Tuning in Logistic Regression is the process of selecting the optimal hyperparameters for a Logistic Regression model to achieve the best predictive performance.\n",
        "\n",
        "####- Hyperparameters in Logistic Regression :\n",
        "1. Regularization strength (C or λ): Controls the amount of regularization applied to the model.\n",
        "2. Penalty type (L1 or L2): Determines the type of regularization used (Lasso or Ridge).\n",
        "3. Max iterations: The maximum number of iterations allowed for the model to converge.\n",
        "4. Tolerance: The convergence threshold for the model.\n",
        "\n",
        "####- Hyperparameter Tuning Techniques :\n",
        "1. Grid Search: Exhaustively searches through a predefined grid of hyperparameters to find the best combination.\n",
        "2. Random Search: Randomly samples hyperparameters from a predefined distribution to find the best combination.\n",
        "3. Bayesian Optimization: Uses Bayesian inference to search for the optimal hyperparameters.\n",
        "4. Cross-Validation: Evaluates the model's performance on unseen data using different hyperparameter combinations.\n"
      ],
      "metadata": {
        "id": "8lq1YvCKDTqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14 :  What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "In Logistic Regression, solvers are algorithms used to optimize the model's parameters. Here are some common solvers:\n",
        "\n",
        "####- Solvers in Logistic Regression :\n",
        "1. Liblinear: A linear solver that uses the LIBLINEAR library. Suitable for small to medium-sized datasets.\n",
        "2. SAGA: A stochastic gradient descent solver that uses the SAGA algorithm. Suitable for large datasets.\n",
        "3. SAG: A stochastic gradient descent solver that uses the SAG algorithm. Similar to SAGA but less efficient.\n",
        "4. Newton-CG: A Newton conjugate gradient solver. Suitable for small to medium-sized datasets.\n",
        "5. LBFGS: A limited-memory BFGS solver. Suitable for small to medium-sized datasets.\n",
        "\n",
        "####- Choosing a Solver :\n",
        "1. Default solver: Liblinear is often the default solver. If you're unsure, start with this.\n",
        "2. Dataset size: For large datasets, use SAGA or SAG. For small datasets, use Liblinear, Newton-CG, or LBFGS.\n",
        "3. Computational resources: If computational resources are limited, use Liblinear or SAGA.\n",
        "4. Convergence speed: If fast convergence is necessary, use Newton-CG or LBFGS.\n",
        "5. Regularization: If regularization is used, Liblinear or SAGA may be a better choice.\n"
      ],
      "metadata": {
        "id": "ejTI2w5yECxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q15 : How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "Logistic Regression can be extended for multiclass classification using the following methods:\n",
        "\n",
        "####- One-vs-Rest (OvR) Method :\n",
        "1. Train multiple binary models: Train a separate Logistic Regression model for each class, where the positive class is the class of interest and the negative class is the combination of all other classes.\n",
        "2. Predict probabilities: Predict the probabilities for each class using the corresponding binary model.\n",
        "3. Choose the class with the highest probability: Select the class with the highest predicted probability as the final prediction.\n",
        "\n",
        "####- One-vs-One (OvO) Method :\n",
        "1. Train multiple binary models: Train a separate Logistic Regression model for each pair of classes.\n",
        "2. Predict probabilities: Predict the probabilities for each class using the corresponding binary model.\n",
        "3. Use voting: Use a voting scheme to determine the final prediction, where each binary model votes for one of the two classes it was trained on.\n",
        "\n",
        "####- Multinomial Logistic Regression :\n",
        "1. Use a softmax activation function: Replace the sigmoid activation function with a softmax activation function, which outputs a probability distribution over all classes.\n",
        "2. Use a cross-entropy loss function: Use a cross-entropy loss function to train the model, which measures the difference between the predicted probabilities and the true labels.\n",
        "\n",
        "Multinomial Logistic Regression is a more straightforward extension of Logistic Regression for multiclass classification, while OvR and OvO are more complex methods that require training multiple models."
      ],
      "metadata": {
        "id": "bD0QclRyE7I2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16 : What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "Here are the advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "####- Advantages :\n",
        "1. Interpretability: Logistic Regression is a simple and interpretable model, making it easy to understand the relationships between the features and the target variable.\n",
        "2. Efficient computation: Logistic Regression is computationally efficient and can be trained on large datasets.\n",
        "3. Robustness to noise: Logistic Regression is robust to noisy data and can handle outliers.\n",
        "4. Easy to implement: Logistic Regression is a widely used algorithm, and its implementation is straightforward in most programming languages.\n",
        "5. Provides probabilities: Logistic Regression provides probabilities for each class, which can be useful in certain applications.\n",
        "\n",
        "####- Disadvantages :\n",
        "1. Assumes linear relationship: Logistic Regression assumes a linear relationship between the features and the log-odds of the target variable, which may not always be the case.\n",
        "2. Not suitable for complex relationships: Logistic Regression is not suitable for modeling complex relationships between features, such as interactions or non-linear effects.\n",
        "3. Sensitive to feature scaling: Logistic Regression is sensitive to feature scaling, and scaling the features can affect the model's performance.\n",
        "4. May not perform well with imbalanced data: Logistic Regression may not perform well with imbalanced data, where one class has a significantly larger number of instances than the other class.\n",
        "5. May not handle high-dimensional data: Logistic Regression may not handle high-dimensional data well, especially when the number of features is larger than the number of instances."
      ],
      "metadata": {
        "id": "n_WxJRgmFcFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q17 : What are some use cases of Logistic Regression?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "Logistic Regression is a versatile algorithm that can be applied to various domains and use cases, including:\n",
        "\n",
        "####- Medical Diagnosis :\n",
        "1. Disease diagnosis: Predict the likelihood of a disease based on symptoms, medical history, and test results.\n",
        "2. Patient risk assessment: Identify high-risk patients for complications or readmissions.\n",
        "\n",
        "####- Finance :\n",
        "1. Credit risk assessment: Predict the likelihood of loan defaults or credit card fraud.\n",
        "2. Insurance risk assessment: Determine premiums based on policyholder characteristics.\n",
        "\n",
        "####- Marketing :\n",
        "1. Customer churn prediction: Identify customers likely to switch to a competitor.\n",
        "2. Lead scoring: Predict the likelihood of converting leads into sales.\n",
        "\n",
        "####- Social Media :\n",
        "1. Sentiment analysis: Classify text as positive, negative, or neutral.\n",
        "2. Spam detection: Identify spam comments or messages.\n",
        "\n",
        "####- Education :\n",
        "1. Student performance prediction: Predict student grades or dropout likelihood.\n",
        "2. Admissions prediction: Determine the likelihood of admission to a university.\n",
        "\n",
        "####- Customer Service :\n",
        "1. Ticket classification: Classify support tickets into categories (e.g., technical, billing).\n",
        "2. Chatbot intent identification: Determine the user's intent behind a chatbot query.\n",
        "\n",
        "####- Sports :\n",
        "1. Game outcome prediction: Predict the likelihood of a team winning a game.\n",
        "2. Player performance prediction: Predict a player's future performance based on past data.\n",
        "\n",
        "These are just a few examples of the many use cases of Logistic Regression. The algorithm can be applied to any problem that involves predicting a binary outcome based on a set of input features."
      ],
      "metadata": {
        "id": "FmtIKVK_FyIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q18 : What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "###ans :\n",
        "\n",
        "Softmax Regression and Logistic Regression are both supervised learning algorithms used for classification tasks. However, they differ in the number of classes they can handle and the output they produce:\n",
        "\n",
        "####- Logistic Regression :\n",
        "1. Binary classification: Logistic Regression is used for binary classification problems, where there are only two classes (e.g., 0 and 1, yes and no).\n",
        "2. Sigmoid output: Logistic Regression uses a sigmoid function to produce a probability output between 0 and 1, indicating the likelihood of belonging to the positive class.\n",
        "3. One-vs-Rest approach: For multi-class problems, Logistic Regression can be extended using the One-vs-Rest approach, where multiple binary models are trained.\n",
        "\n",
        "####- Softmax Regression :\n",
        "1. Multi-class classification: Softmax Regression is used for multi-class classification problems, where there are more than two classes (e.g., handwritten digit recognition).\n",
        "2. Softmax output: Softmax Regression uses a softmax function to produce a probability distribution over all classes, ensuring the probabilities sum to 1.\n",
        "3. Single model: Softmax Regression trains a single model that can handle multiple classes directly, eliminating the need for multiple binary models.\n",
        "\n"
      ],
      "metadata": {
        "id": "0In7FM81GpzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19 : How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "\n",
        "###ans :\n",
        "\n",
        "Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors. Here are some considerations to help us decide:\n",
        "\n",
        "####- One-vs-Rest (OvR) :\n",
        "1. Interpretability: OvR provides separate models for each class, making it easier to interpret the results.\n",
        "2. Flexibility: OvR allows for different classification algorithms to be used for each class.\n",
        "3. Handling imbalanced data: OvR can handle imbalanced data by using class weights or sampling techniques.\n",
        "4. Computational complexity: OvR can be computationally expensive, especially for large datasets.\n",
        "\n",
        "####- Softmax :\n",
        "1. Efficiency: Softmax is generally more efficient than OvR, especially for large datasets.\n",
        "2. Multi-class handling: Softmax is designed for multi-class classification and can handle multiple classes naturally.\n",
        "3. Probabilistic output: Softmax provides a probabilistic output, which can be useful for certain applications.\n",
        "4. Assumes mutual exclusivity: Softmax assumes that the classes are mutually exclusive, which might not always be the case.\n",
        "\n",
        "####- Choosing between OvR and Softmax :\n",
        "1. Number of classes: If there are only a few classes, OvR might be a better choice. For a large number of classes, Softmax might be more efficient.\n",
        "2. Class balance: If the classes are imbalanced, OvR might be a better choice, as it allows for class weights or sampling techniques.\n",
        "3. Interpretability: If interpretability is important, OvR might be a better choice, as it provides separate models for each class.\n",
        "4. Computational resources: If computational resources are limited, Softmax might be a better choice, as it is generally more efficient.\n"
      ],
      "metadata": {
        "id": "bkVQzDA9HDbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q20 : How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "###ans :\n",
        "\n",
        "Interpreting coefficients in Logistic Regression can be a bit tricky, but here's a step-by-step guide:\n",
        "\n",
        "####- Coefficient Interpretation :\n",
        "1. Odds Ratio: The coefficient represents the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, while holding all other predictors constant.\n",
        "2. Log-Odds: The log-odds are the natural logarithm of the odds of the outcome variable. The odds are the ratio of the probability of the outcome variable being true to the probability of it being false.\n",
        "3. Positive and Negative Coefficients: A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome variable. A negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome variable.\n",
        "\n",
        "####- Calculating Odds Ratios :\n",
        "1. Exponentiate the Coefficient: To calculate the odds ratio, exponentiate the coefficient (i.e., e^coefficient).\n",
        "2. Interpret the Odds Ratio: An odds ratio greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome variable. An odds ratio less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome variable.\n",
        "\n",
        "####- Example :\n",
        "Suppose we have a logistic regression model predicting the probability of a person having a heart attack based on their age. The coefficient for age is 0.05.\n",
        "\n",
        "1. Interpret the Coefficient: For every one-year increase in age, the log-odds of having a heart attack increase by 0.05.\n",
        "2. Calculate the Odds Ratio: Exponentiate the coefficient: e^0.05 ≈ 1.05.\n",
        "3. Interpret the Odds Ratio: For every one-year increase in age, the odds of having a heart attack increase by 5%.\n"
      ],
      "metadata": {
        "id": "Wup2wltTHp3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\"Practical Questions and Answers : \""
      ],
      "metadata": {
        "id": "8DC8lE5fA01E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Loading the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Spliting the dataset into features (X) and target (y)\n",
        "X = data.data # Changed from iris.data to data.data\n",
        "y = data.target # Changed from iris.target to data.target\n",
        "\n",
        "# Spliting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KPRnl7w-Bd--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_iris()"
      ],
      "metadata": {
        "id": "llHijxv0BFZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1 : Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "OFpn3yhlBCIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans:\n",
        "\n",
        "# Creating a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Training the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#calculating the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8-h5lznByfk",
        "outputId": "65c2f6f7-1ce2-4ee6-9916-ada0a153e79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2 : Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy."
      ],
      "metadata": {
        "id": "qK3s2DReWs_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# Creating a Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Training the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#the model accuracy\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# the coefficients of the model\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkOD_EnTXK8K",
        "outputId": "2f634be7-da06-4dc1-891b-ab1ef596d61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Model Coefficients: [[ 0.          2.36286687 -2.6717545   0.        ]\n",
            " [ 0.69336361 -1.78834497  0.24509862 -0.89010743]\n",
            " [-2.20234119 -2.83339142  3.20375711  3.72134126]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3 : Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "BgibLHqPaa8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# Creating a Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "\n",
        "# Training the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# the model accuracy\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# the coefficients of the model\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44JRlxncahq8",
        "outputId": "705ca6aa-4e83-4fcc-939e-06d6841319ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Model Coefficients:\n",
            " [[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4 : Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "3GKeZE_fbx4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "#Creating a Logistic Regression model with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, l1_ratio=0.5)\n",
        "\n",
        "# Training the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# model accuracy\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# coefficients of the model\n",
        "print(\"Model Coefficients:\\n\", model.coef_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1t_Q2P-b4NK",
        "outputId": "56ca2e18-2a30-4fce-a6fb-c2ca00d1ecc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Model Coefficients:\n",
            " [[ 0.02831438  1.59695199 -2.42639831 -0.59307427]\n",
            " [ 0.          0.          0.         -0.50457075]\n",
            " [-0.78010806 -0.95700738  2.74511882  2.09727486]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5 : Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'."
      ],
      "metadata": {
        "id": "YF_uZe6zcXGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# Creating a Logistic Regression model for multiclass classification using one-vs-rest\n",
        "model = LogisticRegression(max_iter=1000, multi_class='ovr')\n",
        "\n",
        "# Training the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# model accuracy\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns-TWoc8cdaJ",
        "outputId": "585f90e1-d29b-4499-d55a-a24fe18268e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6 : Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "oPa9eSi-dpSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Creating a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "\n",
        "# Applying GridSearchCV to tune the hyperparameters\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# the best accuracy\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluating the best model on the testing set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Testing Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiONF6XXdwZT",
        "outputId": "c15644d3-c4a3-47bf-fd02-9e4672a7c67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Accuracy: 0.9583333333333334\n",
            "Accuracy on Testing Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7 : Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "ioM64x6tj65f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "#Here's a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation:\n",
        "# Creating a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initializing a list to store the accuracy scores\n",
        "accuracies = []\n",
        "\n",
        "# Initializing StratifiedKFold\n",
        "kfold = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterating over the folds\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    # Spliting the data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Training the model on the training set\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # predicting the testing set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculating the accuracy score\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Appending the accuracy score to the list\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# average accuracy\n",
        "average_accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "# average accuracy\n",
        "print(\"Average Accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3nq5hltGKOu",
        "outputId": "d97ccc77-e8ae-4914-afcc-7c1b6a4f972a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q8 : Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "qZ6Xe-By-0mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "import zipfile\n",
        "\n",
        "# Specifying the path to the zip file\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "\n",
        "# Specifying the path to the CSV file within the zip file\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "# Extracting the CSV file from the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "  with z.open(csv_file_path) as f:\n",
        "    # the dataset\n",
        "    df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical, using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and target variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "6zBzQ1Ktay1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a95feda-a8e6-4615-b285-6ee6bc4e42e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9098082058752124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q9 : Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "RjGaQtXvbzcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Defining the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Creating a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Applying RandomizedSearchCV for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(model, param_grid, cv=5, n_iter=10, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# the best accuracy\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluating the best model on the testing set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Testing Set:\", accuracy)"
      ],
      "metadata": {
        "id": "Yjx4LJ0HeU-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4deae65-a65f-4c41-acbb-c5c57053bf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 10}\n",
            "Best Accuracy: 0.9666666666666668\n",
            "Accuracy on Testing Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q10 : Write a Python program to implement One-vs-One(OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "XwyaoGmcdZva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# Creating a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Defining the number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "# Initializing a list to store the OvO models\n",
        "ovo_models = []\n",
        "\n",
        "# Training OvO models\n",
        "for i in range(n_classes):\n",
        "    for j in range(i+1, n_classes):\n",
        "        # Creating a binary target variable\n",
        "        y_train_binary = np.where(y_train == i, 0, np.where(y_train == j, 1, 2))\n",
        "        y_test_binary = np.where(y_test == i, 0, np.where(y_test == j, 1, 2))\n",
        "\n",
        "        # Creating a Logistic Regression model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        # Training the model on the binary target variable\n",
        "        model.fit(X_train, y_train_binary)\n",
        "\n",
        "        # Appending the model to the list\n",
        "        ovo_models.append((i, j, model))\n",
        "\n",
        "# predicting the use the OvO models\n",
        "y_pred = np.zeros(len(y_test))\n",
        "for i in range(len(y_test)):\n",
        "    votes = np.zeros(n_classes)\n",
        "    for model in ovo_models:\n",
        "        pred = model[2].predict(X_test[i].reshape(1, -1))\n",
        "        if pred[0] == 0:\n",
        "            votes[model[0]] += 1\n",
        "        else:\n",
        "            votes[model[1]] += 1\n",
        "    y_pred[i] = np.argmax(votes)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6j1xlKMdfV3",
        "outputId": "03fe3c58-6199-4c77-a44c-a05558ab987d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q11 : Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification."
      ],
      "metadata": {
        "id": "tRvon7S0f8zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and target variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Computing the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizing the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "On4EREXvgFkt",
        "outputId": "703bf907-8438-4056-8b61-b439c3d9d36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q12 : Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "VaeMINjhhQ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "import zipfile\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features (X) and target variable (y)\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# the results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0uJglhlhZHj",
        "outputId": "1ce781a8-1af3-4f79-9abc-f129eb7f23b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6589\n",
            "Recall: 0.4257\n",
            "F1-Score: 0.5172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q13 : Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance."
      ],
      "metadata": {
        "id": "5v-JNT30iU_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "import zipfile\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical using one-hot encoding\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features (X) and target variable (y)\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes= np.array([0,1]), y=y_train)\n",
        "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Training the Logistic Regression model with class weights\n",
        "model_weighted = LogisticRegression(max_iter=1000, class_weight = class_weights_dict)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "# Evaluating performance with class weights\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "report_weighted = classification_report(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"Model with Class Weights:\")\n",
        "print(f\"Accuracy: {accuracy_weighted:.4f}\")\n",
        "print(\"Classification Report:\\n\", report_weighted)\n",
        "\n",
        "# Training Logistic Regression model without class weights\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating performance without class weights\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"\\nModel without Class Weights:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM7nGKMAjJAs",
        "outputId": "00e69746-396a-42e0-ee87-329b36edad0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with Class Weights:\n",
            "Accuracy: 0.8557\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.98      0.85      0.91      7303\n",
            "        True       0.43      0.89      0.58       935\n",
            "\n",
            "    accuracy                           0.86      8238\n",
            "   macro avg       0.71      0.87      0.75      8238\n",
            "weighted avg       0.92      0.86      0.88      8238\n",
            "\n",
            "\n",
            "Model without Class Weights:\n",
            "Accuracy: 0.9098\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.97      0.95      7303\n",
            "        True       0.66      0.43      0.52       935\n",
            "\n",
            "    accuracy                           0.91      8238\n",
            "   macro avg       0.79      0.70      0.73      8238\n",
            "weighted avg       0.90      0.91      0.90      8238\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q14 : Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance."
      ],
      "metadata": {
        "id": "sttJR413jPz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic_df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "# missing values\n",
        "# For numerical columns, impute with the mean\n",
        "numerical_cols = ['Age', 'Fare']\n",
        "for col in numerical_cols:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    titanic_df[col] = imputer.fit_transform(titanic_df[[col]])\n",
        "\n",
        "\n",
        "# For categorical columns, impute with the most frequent value\n",
        "categorical_cols = ['Embarked']\n",
        "for col in categorical_cols:\n",
        "    imputer = SimpleImputer(strategy='most_frequent')\n",
        "    #access the first column of the transformed numpy array\n",
        "    titanic_df[col] = imputer.fit_transform(titanic_df[[col]])[:,0]\n",
        "\n",
        "\n",
        "# Convert categorical features into numerical\n",
        "label_encoders = {}\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    label_encoders[feature] = LabelEncoder()\n",
        "    titanic_df[feature] = label_encoders[feature].fit_transform(titanic_df[feature])\n",
        "\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "\n",
        "X = titanic_df[features]\n",
        "y = titanic_df[target]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# the results\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBl1FxexkVZW",
        "outputId": "522d0a66-18a0-48b8-a1a0-982369d672e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Model Accuracy: 0.8101\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q15 : Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "6Y5kFxCNlMKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Without Feature Scaling ---\n",
        "# Initialize and train the Logistic Regression model\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without Feature Scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# --- With Feature Scaling (Standardization) ---\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with Feature Scaling:    {accuracy_scaled:.4f}\")\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Accuracy Difference: {accuracy_scaled - accuracy_no_scaling:.4f}\")\n",
        "\n",
        "if accuracy_scaled > accuracy_no_scaling:\n",
        "    print(\"Feature scaling improved the model accuracy.\")\n",
        "elif accuracy_scaled < accuracy_no_scaling:\n",
        "    print(\"Feature scaling reduced the model accuracy.\")\n",
        "else:\n",
        "    print(\"Feature scaling did not change the model accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaYe11_plTNo",
        "outputId": "dc854b35-bbfd-4a28-e110-5b613634e487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Feature Scaling: 1.0000\n",
            "Accuracy with Feature Scaling:    1.0000\n",
            "\n",
            "Comparison:\n",
            "Accuracy Difference: 0.0000\n",
            "Feature scaling did not change the model accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q16 :  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "n8SbeYjeXNCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting the categorical variables to numerical\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and targeting variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting probabilities for the test set\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "#  ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "JfaryraxXUwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844f34a7-a6c5-4c54-c9f4-76536f7d7483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9307844333256934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q17 : Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "ijPPcEf0l8bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and target variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and training the Logistic Regression model with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UzcZB1NmCnp",
        "outputId": "adf909bb-ad43-4c21-cefa-bbc1620a0bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.911143481427531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q18 : Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "a5BEKA3GmMSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Converting categorical variables to numerical\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating the features and targeting variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# feature names\n",
        "feature_names = X.columns\n",
        "\n",
        "# model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Creating a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
        "\n",
        "# Sort by absolute coefficient values\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-RmN6DsmTGy",
        "outputId": "3feed6fb-9dd2-4cc9-fe86-c8233b606b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Feature  Coefficient  Abs_Coefficient\n",
            "43                      month_may    -1.016163         1.016163\n",
            "42                      month_mar     0.591963         0.591963\n",
            "37              contact_telephone    -0.436427         0.436427\n",
            "6                  cons.price.idx     0.387653         0.387653\n",
            "5                    emp.var.rate    -0.386998         0.386998\n",
            "51           poutcome_nonexistent     0.292720         0.292720\n",
            "31                default_unknown    -0.292502         0.292502\n",
            "40                      month_jul     0.261071         0.261071\n",
            "38                      month_aug     0.246226         0.246226\n",
            "41                      month_jun     0.241277         0.241277\n",
            "10                job_blue-collar    -0.223868         0.223868\n",
            "4                        previous    -0.212658         0.212658\n",
            "44                      month_nov    -0.210014         0.210014\n",
            "14                    job_retired     0.166729         0.166729\n",
            "47                day_of_week_mon    -0.155408         0.155408\n",
            "29    education_university.degree     0.146338         0.146338\n",
            "50                day_of_week_wed     0.134198         0.134198\n",
            "49                day_of_week_tue     0.133811         0.133811\n",
            "17                    job_student     0.116949         0.116949\n",
            "46                      month_sep    -0.108469         0.108469\n",
            "16                   job_services    -0.094371         0.094371\n",
            "15              job_self-employed    -0.090636         0.090636\n",
            "52               poutcome_success     0.082197         0.082197\n",
            "18                 job_technician     0.079449         0.079449\n",
            "8                       euribor3m    -0.076750         0.076750\n",
            "2                        campaign    -0.075408         0.075408\n",
            "45                      month_oct     0.070228         0.070228\n",
            "25             education_basic.9y    -0.067867         0.067867\n",
            "11               job_entrepreneur    -0.051923         0.051923\n",
            "36                       loan_yes    -0.046226         0.046226\n",
            "22                 marital_single     0.040376         0.040376\n",
            "13                 job_management    -0.039012         0.039012\n",
            "26          education_high.school    -0.038814         0.038814\n",
            "19                 job_unemployed     0.034152         0.034152\n",
            "48                day_of_week_thu     0.030176         0.030176\n",
            "12                  job_housemaid    -0.027630         0.027630\n",
            "34                    housing_yes    -0.025434         0.025434\n",
            "20                    job_unknown    -0.017887         0.017887\n",
            "30              education_unknown     0.012924         0.012924\n",
            "7                   cons.conf.idx     0.010137         0.010137\n",
            "21                marital_married    -0.008157         0.008157\n",
            "9                     nr.employed    -0.007284         0.007284\n",
            "27           education_illiterate     0.005325         0.005325\n",
            "1                        duration     0.004702         0.004702\n",
            "39                      month_dec    -0.004111         0.004111\n",
            "0                             age    -0.003793         0.003793\n",
            "23                marital_unknown    -0.002638         0.002638\n",
            "28  education_professional.course     0.002216         0.002216\n",
            "3                           pdays    -0.001806         0.001806\n",
            "35                   loan_unknown     0.001265         0.001265\n",
            "33                housing_unknown     0.001265         0.001265\n",
            "24             education_basic.6y     0.000760         0.000760\n",
            "32                    default_yes    -0.000296         0.000296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q19 : Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score."
      ],
      "metadata": {
        "id": "gizn64ydmjh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and targeting variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11O4xtUCmqUS",
        "outputId": "c0724c09-8c35-4055-9ca5-a8b296d08270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.4700026894564475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q20 : Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification."
      ],
      "metadata": {
        "id": "x0QLw3Efm6R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import zipfile\n",
        "\n",
        "#  the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separating features and targeting variable\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing and training the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "average_precision = average_precision_score(y_test, y_prob)\n",
        "\n",
        "# Ploting the precision-recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall Curve (Avg Precision = {average_precision:.2f})')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ADN7Vw8UnDeq",
        "outputId": "39905e19-02d0-4f15-efbc-076b69f9d99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q21 : Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "7aoe2mSanQlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining the solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Loop through each solver\n",
        "for solver in solvers:\n",
        "    # Initializing and training the Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKPn7dW5nWyh",
        "outputId": "7a257a5d-f88c-439e-be12-7c06a8b403e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.9090798737557659\n",
            "Solver: saga, Accuracy: 0.9068948773974266\n",
            "Solver: lbfgs, Accuracy: 0.9098082058752124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q22 : Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "LXqQdsTvnrUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Convertion\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separation\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JGd_wF7nyWI",
        "outputId": "28a0f4df-af68-4db6-821d-79b45d26cdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.4836779399526273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q23 : Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "1uGeBICIoLwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Convertion\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separation\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Training on Raw Data ---\n",
        "# Initialize and train the Logistic Regression model on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "\n",
        "# predictions on the test set with raw data\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "\n",
        "# Evaluating accuracy on raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "print(f\"Accuracy with Raw Data: {accuracy_raw}\")\n",
        "\n",
        "# --- Training on Standardized Data ---\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# predictions on the test set with standardized data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy with Standardized Data: {accuracy_scaled}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfT2NH0GoVWH",
        "outputId": "d15005b0-33ab-4a1d-edce-b78720490278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Raw Data: 0.9098082058752124\n",
            "Accuracy with Standardized Data: 0.9115076474872542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q24 :  Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation."
      ],
      "metadata": {
        "id": "M3LtEmSGoqH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "# Defining a range of C values to test\n",
        "C_values = np.logspace(-4, 4, 10) # 10 values from 10^-4 to 10^4\n",
        "\n",
        "# Initializing the Stratified K-Fold\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_c = None\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "for C in C_values:\n",
        "    # Initializing the logistic regression model\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "\n",
        "    # Performing cross-validation\n",
        "    scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "\n",
        "    mean_accuracy = scores.mean()\n",
        "    accuracies.append(mean_accuracy)\n",
        "\n",
        "    # Checking if the current C has the best accuracy so far\n",
        "    if mean_accuracy > best_accuracy:\n",
        "        best_accuracy = mean_accuracy\n",
        "        best_c = C\n",
        "\n",
        "print(f\"Optimal C: {best_c}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWl4emTlowTT",
        "outputId": "d084ce55-5c1a-493d-d604-26c6b1ed4c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 21.54434690031882\n",
            "Best Cross-Validation Accuracy: 0.9105565243169524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q25 : Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "shVB0t06pGYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans :\n",
        "import joblib\n",
        "import zipfile\n",
        "\n",
        "# the dataset\n",
        "zip_file_path = '/content/bank-additional-full.csv.zip'\n",
        "csv_file_path = 'bank-additional-full.csv'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
        "    with z.open(csv_file_path) as f:\n",
        "        df = pd.read_csv(f, sep=';')\n",
        "\n",
        "# Convertion\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separation\n",
        "X = df.drop('y_yes', axis=1)\n",
        "y = df['y_yes']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- Saving the trained model using joblib ---\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Trained model saved to {model_filename}\")\n",
        "\n",
        "# --- Loading the saved model using joblib ---\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions using loaded model:\", y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZQvTLfBpPEQ",
        "outputId": "662d3b6c-620b-4bd8-c1dc-6453c8293727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved to logistic_regression_model.joblib\n",
            "Model loaded successfully.\n",
            "Predictions using loaded model: [False False False ... False  True False]\n"
          ]
        }
      ]
    }
  ]
}